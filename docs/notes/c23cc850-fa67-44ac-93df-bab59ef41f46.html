<!DOCTYPE html>

<html lang="en-US"><head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
  
  <title>Algorithmic Fairness - Angela's note universe</title>
  

  <link
    rel="shortcut icon"
    href="https://angelarwang.com/dendron-notes/favicon.ico"
    type="image/x-icon"
  />
  <link
    rel="stylesheet"
    href="https://angelarwang.com/dendron-notes/assets/css/just-the-docs-default.css"
  />
  <script
    type="text/javascript"
    src="https://angelarwang.com/dendron-notes/assets/js/just-the-docs.js"
  ></script>
  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.22.0/themes/prism.min.css"
  />
  <link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
    crossorigin="anonymous"
  />
  <script
    type="text/javascript"
    src="https://angelarwang.com/dendron-notes/assets/js/vendor/lunr.min.js"
  ></script>
  
  <meta name="viewport" content="width=device-width, initial-scale=1" />
   
  
<meta name="description" content="This is where I keep and share my notes on books, tech, travel, and other hobbies :)">
<meta name="author" content="[object Object]">

<link rel="canonical" href="https://angelarwang.com/notes/c23cc850-fa67-44ac-93df-bab59ef41f46.html">

<meta property="og:type" content="article">
<meta property="og:url" content="https://angelarwang.com/notes/c23cc850-fa67-44ac-93df-bab59ef41f46.html">
<meta property="og:description" content="This is where I keep and share my notes on books, tech, travel, and other hobbies :)">
<meta property="og:image" content="https://angelarwang.com/logo.png">

<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@{"card":"summary","username":"angelaw_ruohan"}">
<meta name="twitter:url" content="https://angelarwang.com/notes/c23cc850-fa67-44ac-93df-bab59ef41f46.html">
<meta name="twitter:description" content="This is where I keep and share my notes on books, tech, travel, and other hobbies :)">
<meta name="twitter:image" content="https://angelarwang.com/logo.png">


  <meta property="og:title" content="Algorithmic Fairness - Angela's note universe" />
  <meta name="twitter:title" content="Algorithmic Fairness - Angela's note universe" />
  
</head>


<body style="">
    <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
      <symbol id="svg-link" viewBox="0 0 24 24">
        <title>Link</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
          <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
        </svg>
      </symbol>
      <symbol id="svg-search" viewBox="0 0 24 24">
        <title>Search</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
          <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
        </svg>
      </symbol>
      <symbol id="svg-menu" viewBox="0 0 24 24">
        <title>Menu</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
          <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
        </svg>
      </symbol>
      <symbol id="svg-arrow-right" viewBox="0 0 24 24">
        <title>Expand</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
          <polyline points="9 18 15 12 9 6"></polyline>
        </svg>
      </symbol>
      <symbol id="svg-doc" viewBox="0 0 24 24">
        <title>Document</title>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
          <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
        </svg>
      </symbol>
    </svg>

    <div class="side-bar">
      <div class="site-header">
        <a href="https://angelarwang.com/dendron-notes" class="site-title lh-tight">
  <div class="site-logo"></div>

 </a>
        <a href="#" id="menu-button" class="site-button">
          <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg>
        </a>
      </div>

      <nav role="navigation" aria-label="Main" id="site-nav" class="site-nav">
      </nav>
      <footer class="site-footer">
        ðŸŒ± with ðŸ’• using <a href="https://www.dendron.so/"> Dendron ðŸŒ² </a>
      </footer>
    </div>
    <div class="main" id="top">
        <div id="main-header" class="main-header">
          
            <div class="search">
              <div class="search-input-wrap">
                <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Angela's note universe" aria-label="Search Angela's note universe" autocomplete="off">
                <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
              </div>
              <div id="search-results" class="search-results"></div>
            </div>
          
          
    </div>
    <div id="main-content-wrap" class="main-content-wrap">
      
        
          <nav aria-label="Breadcrumb" class="breadcrumb-nav">
            <ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="https://angelarwang.com/dendron-notes">Root</a></li><li class="breadcrumb-nav-list-item"><a href="https://angelarwang.com/dendron-notes/notes/bee70da8-e282-4d98-9709-1bad45e8a734.html">Topics</a></li><li class="breadcrumb-nav-list-item"><a href="https://angelarwang.com/dendron-notes/notes/056961d3-aba0-4fc6-ba6f-598dbc38294c.html">Machine Learning</a></li><li class="breadcrumb-nav-list-item">Algorithmic Fairness</li></ol>
          </nav>
        
      
      <div id="main-content" class="main-content" role="main">

        
        

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

        

        <script>
          $(function () {
            $('[data-toggle="popover"]').popover({html: true})
          })
        </script>

        

<div id="main" role="main">

  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Algorithmic Fairness">
    
    <meta itemprop="datePublished" content="2020-12-09T13:05:51+00:00">
    <meta itemprop="dateModified" content="2021-02-28T13:30:39+00:00">

    <div class="page__inner-wrap">

      

      <section class="page__content" itemprop="text">

        

        <h1 id="algorithmic-fairness"><a aria-hidden="true" class="anchor-heading" href="#algorithmic-fairness"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Algorithmic Fairness</h1>
<h3 id="algorithmic-fairness-from-a-non-ideal-perspective"><a aria-hidden="true" class="anchor-heading" href="#algorithmic-fairness-from-a-non-ideal-perspective"><svg aria-hidden="true" viewBox="0 0 16 16"><use xlink:href="#svg-link"></use></svg></a>Algorithmic Fairness from a Non-ideal Perspective</h3>
<ul>
<li>author/speaker: Zack Lipton  </li>
<li><a href="https://arxiv.org/abs/2001.09773">https://arxiv.org/abs/2001.09773</a> </li>
</ul>
<ul>
<li>
<p>important moments:</p>
<ul>
<li>ProPublica machine bias article from 2016 - a galvanizing event prompting interest: <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a> </li>
<li><a href="http://proceedings.mlr.press/v81/buolamwini18a.html">Gender Shades</a>- 2018. unbalanced dataset of facial benchmarks </li>
<li>biased allocation of healthcare - <a href="https://www.theverge.com/2019/10/24/20929337/care-algorithm-study-race-bias-health">https://www.theverge.com/2019/10/24/20929337/care-algorithm-study-race-bias-health</a> </li>
</ul>
</li>
<li>
<p>Philosophy of justice </p>
<ul>
<li><a href="https://plato.stanford.edu/entries/justice/">https://plato.stanford.edu/entries/justice/</a>
<ul>
<li>obligation ("one's due") in contrast to charity</li>
<li>resolve conflicts when interests clash</li>
<li>impartiality - two cases relvantly alike should be treated similarly </li>
</ul>
</li>
<li>convservative (conserving existing norms) vs ideal (demand reform of norms and practicies) </li>
<li>corrective (bilateral, wrong-doer &#x3C;> wronged ) vs distributive (allocating goods to individuals, mutltilateral with a distributing agent)</li>
<li>procedural (procedure) vs substantive (end result)</li>
<li>comparative (a position was offered to a less qualified candidate) vs non-comparative (regardless of comparison, e.g. basic human rights)
<ul>
<li>ML disccussion often focus on comparative. should also consider non-comparative</li>
</ul>
</li>
<li>scope of justice </li>
<li>ideal and non-ideal theorizing (The Imperative of Integration - Elizabeth S. Anderson)
<ul>
<li>ideal:
<ul>
<li>imagine perfectly just world  (Rawls)</li>
<li>try to minimzie discrepancy between reality and ideal</li>
<li>e.g. been used to argue against affirmative acction - ideal world is color blind </li>
</ul>
</li>
<li>non-ideal:
<ul>
<li>understand causual explanation of the problem, determine waht can be done and who to correct it. </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>economics </p>
<ul>
<li>"the economics of discrimination" - Becker 
<ul>
<li>simulation of employer taste-based discrimination</li>
</ul>
</li>
<li>Arrow's Rebuttal to Becker 1973 
<ul>
<li>imperfect information as alternative cause </li>
</ul>
</li>
<li>"Statistical theory of racism and sexism"
<pre><code>   - Efficient candidate screening under multiple tests and implications for fairness
</code></pre>
<a href="https://arxiv.org/abs/1905.11361">https://arxiv.org/abs/1905.11361</a>
<pre><code>       - concerns noise 
       - by adjusting number of interviews. hitting boundary on left, can equalize FP and FN across groups w different noise level 
           - but more num of interviews can negatively affect candidates too 
   - Will Affirmative-Action Policies Eliminate Negative Stereotypes?
       - Stephen Coate and Glenn C. Loury
       - https://www.brown.edu/Departments/Economics/Faculty/Glenn_Loury/louryhomepage/papers/Coate%20and%20Loury%20(AER%201993).pdf
       - even when groups are equal ex ante, equilibrium outcomes following some internventions can appear to confirm negative sterotypes 
</code></pre>
</li>
</ul>
</li>
<li>
<p>ML fairness</p>
<ul>
<li>
<p><a href="https://www.theatlantic.com/technology/archive/2018/05/machine-learning-is-stuck-on-asking-why/560675/?utm_source=twb">https://www.theatlantic.com/technology/archive/2018/05/machine-learning-is-stuck-on-asking-why/560675/?utm_source=twb</a> </p>
<ul>
<li>"ML is stuck on ... learning associations "</li>
<li>it learns associations and not causal relations </li>
<li>on one hand, curve fitting turned out useful many places</li>
<li>but in many problems, curve fitting is not enough</li>
</ul>
</li>
<li>
<p><a href="http://approximatelycorrect.com/2016/11/07/the-foundations-of-algorithmic-bias/">http://approximatelycorrect.com/2016/11/07/the-foundations-of-algorithmic-bias/</a> </p>
</li>
<li>
<p>taking inspiration from law: </p>
<ul>
<li>title 7 of civil rights law 
<ul>
<li>disparate treatment 
<ul>
<li>addresses <strong>intentional</strong> discrimination
<ul>
<li>protected characterstic </li>
<li>also via proxy variables </li>
<li>with exceptions e.g. if goal is to promote diversity</li>
</ul>
</li>
<li>what does intention mean in ML context? </li>
</ul>
</li>
<li>disparate treament   
<ul>
<li>3 tests:
<ul>
<li>plaintiff must demo statistical disparity (4/5 rule)</li>
<li>defendent must show descirions are justified by business necessity </li>
<li>plaintiff must show defendent can achieve goal w alternative practice </li>
</ul>
</li>
<li>first one can be done using stats. </li>
<li>later 2 of the 3 tests of the above are not well adressed by ML, require causal reasoning </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>typical </p>
<ul>
<li>treatment parity
<ul>
<li>output does not depend on sensitive characteristic
<ul>
<li>Model cannot use that feature </li>
</ul>
</li>
</ul>
</li>
<li>impact parity 
<ul>
<li>outcome independent of group status
<ul>
<li>model can use the feature, but result algo outcome is indenpdent
<img src="/dendron-notes/assets/images/2020-12-10-16-11-01.png"></li>
</ul>
</li>
</ul>
</li>
<li>representational parity 
<ul>
<li>the input map to some representation that you can't infer their demographics </li>
<li>entails impact parity</li>
</ul>
</li>
<li>equalized odds / opportunity parity 
<ul>
<li>equal FN and/or FP rates </li>
</ul>
</li>
</ul>
</li>
<li>
<p>problem </p>
<ul>
<li>the different parities are mutually irreconcilable </li>
<li>statisctical parity may not capture legal/philosophical notions </li>
<li>lack ingredients to determine just action
<ul>
<li>how did disparities arive</li>
<li>impact of the decision</li>
<li>responsibilities of the decision maker </li>
</ul>
</li>
</ul>
</li>
<li>
<p>"impossibility" theorem: </p>
<ul>
<li>if we start from a <strong>non-ideal world</strong>, no set of action can simultaneously satisfy all the ideal </li>
<li>meeting the ideal in some respect may require widening other gaps </li>
<li>"equity" - peyton young, 1994
<pre><code>- different definition of equity which all seem reasonable by itself, when together causes non-reconcible conflict 
</code></pre>
=> must make some choices </li>
</ul>
</li>
<li>
<p>problem applications of attempts for fair algorithm </p>
<ul>
<li>
<p><img src="/dendron-notes/assets/images/2020-12-10-16-20-04.png"></p>
<ul>
<li>
<p>for maximizing impact disparity, treatment disparity is optimal (theortical)</p>
</li>
<li>
<p>if other features sufficiently can encode the sensitive feature, result is indistinguishable from teatment disparity (theortical)</p>
</li>
<li>
<p>if other features partially encode sensitive feature => empirical side effects</p>
<ul>
<li>recorders within group that makes no (not procedurally justifiable)</li>
<li>produces potentially bizarre incentives to conform to steortype </li>
</ul>
</li>
<li>
<p>example case study of gender study in CS admissions</p>
<ul>
<li>when applied DLP, the decisions were flipped neg to pos, for candidates that based on other traits were more likely to be female and vice versa 
<ul>
<li>in effect hurt female candidates who were applying to fields that are more male dominated </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>interesting research to follow</p>
<ul>
<li>causal approaches to fairness
<ul>
<li>counterfactual fairness (Kusner 2017)</li>
<li>causal explanation (bareinboim 2017)</li>
<li>sensitive to subjectivity (different interpretations of the cause)</li>
<li>outsource the key issue to humans </li>
</ul>
</li>
<li>feedback loops - next-step or equilibrium outcomes in a dynamic model 
<ul>
<li>Delayed impact of fair ML, Liu et al</li>
<li>Social Cost of strategic classification / Disparate effects of Strategic manipulation</li>
<li>Runaway Feedback Loops in predictive Policing</li>
</ul>
</li>
</ul>
</li>
</ul><span id="navId" data="c23cc850-fa67-44ac-93df-bab59ef41f46"></span>



                
      </section>

      
    </div>

    
  </article>

  
  
</div>


        
          <hr>
          <footer>
            
            

            
                
                  <p class="text-small text-grey-dk-000 mb-0">
                    <a href="https://github.com/angelarw/dendron-notes/edit/gh-pages/vault/topics.ml.algorithmic-fairness.md" id="edit-this-page">Edit this page on GitHub</a>
                  </p>
                
              </div>
          </footer>
        
    </div>
</div>


  

  <div class="search-overlay"></div>

</div>
</body>
</html>
